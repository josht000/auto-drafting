06/13/2024 15:36:24 - INFO - __main__ - Distributed environment: DistributedType.NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
Mixed precision type: no
{'sample_max_value', 'variance_type', 'thresholding', 'clip_sample_range', 'timestep_spacing', 'rescale_betas_zero_snr', 'prediction_type', 'dynamic_thresholding_ratio'} was not found in config. Values will be initialized to default values.
{'latents_mean', 'scaling_factor', 'latents_std', 'force_upcast'} was not found in config. Values will be initialized to default values.
{'class_embed_type', 'time_cond_proj_dim', 'encoder_hid_dim', 'upcast_attention', 'addition_embed_type', 'encoder_hid_dim_type', 'dropout', 'attention_type', 'mid_block_type', 'resnet_out_scale_factor', 'transformer_layers_per_block', 'only_cross_attention', 'cross_attention_norm', 'num_class_embeds', 'resnet_skip_time_act', 'projection_class_embeddings_input_dim', 'use_linear_projection', 'addition_embed_type_num_heads', 'time_embedding_act_fn', 'class_embeddings_concat', 'resnet_time_scale_shift', 'dual_cross_attention', 'conv_in_kernel', 'reverse_transformer_layers_per_block', 'addition_time_embed_dim', 'time_embedding_type', 'time_embedding_dim', 'mid_block_only_cross_attention', 'timestep_post_act', 'num_attention_heads', 'conv_out_kernel'} was not found in config. Values will be initialized to default values.
C:\Users\see\anaconda3\envs\Bachelor\Lib\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
C:\Users\see\anaconda3\envs\Bachelor\Lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
06/13/2024 15:36:27 - INFO - __main__ - ***** Running training *****
06/13/2024 15:36:27 - INFO - __main__ -   Num Epochs = 215
06/13/2024 15:36:27 - INFO - __main__ -   Instantaneous batch size per device = 4
06/13/2024 15:36:27 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4
06/13/2024 15:36:27 - INFO - __main__ -   Gradient Accumulation steps = 1
06/13/2024 15:36:27 - INFO - __main__ -   Total optimization steps = 15000
Steps:   0%|                                                                                                                                                                                                      | 0/15000 [00:00<?, ?it/s]
Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]
Steps:   0%|                                                                                                                                                                                                      | 0/15000 [00:00<?, ?it/s]C:\Users\see\anaconda3\envs\Bachelor\Lib\site-packages\diffusers\models\attention_processor.py:1406: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:455.)
  hidden_states = F.scaled_dot_product_attention(
C:\Users\see\anaconda3\envs\Bachelor\Lib\site-packages\torch\nn\modules\conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\cudnn\Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
Steps:   0%|                                                                                                                                                              | 1/15000 [00:08<34:12:26,  8.21s/it, lr=0.0001, step_loss=0.0338]Traceback (most recent call last):
  File "C:\Users\see\Documents\GitHub\Diffusion-Models-for-floor-plan-drafting\lora_training.py", line 483, in <module>
    main()
  File "C:\Users\see\Documents\GitHub\Diffusion-Models-for-floor-plan-drafting\lora_training.py", line 284, in main
    model_pred = unet(noisy_latents, timesteps, encoder_hidden_states, return_dict=False)[0]
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\see\anaconda3\envs\Bachelor\Lib\site-packages\torch\nn\modules\module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\see\anaconda3\envs\Bachelor\Lib\site-packages\torch\nn\modules\module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\see\anaconda3\envs\Bachelor\Lib\site-packages\diffusers\models\unets\unet_2d_condition.py", line 1285, in forward
    sample = upsample_block(
             ^^^^^^^^^^^^^^^
  File "C:\Users\see\anaconda3\envs\Bachelor\Lib\site-packages\torch\nn\modules\module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\see\anaconda3\envs\Bachelor\Lib\site-packages\torch\nn\modules\module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\see\anaconda3\envs\Bachelor\Lib\site-packages\diffusers\models\unets\unet_2d_blocks.py", line 2551, in forward
    hidden_states = attn(
                    ^^^^^
  File "C:\Users\see\anaconda3\envs\Bachelor\Lib\site-packages\torch\nn\modules\module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\see\anaconda3\envs\Bachelor\Lib\site-packages\torch\nn\modules\module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\see\anaconda3\envs\Bachelor\Lib\site-packages\diffusers\models\transformers\transformer_2d.py", line 448, in forward
    hidden_states = block(
                    ^^^^^^
  File "C:\Users\see\anaconda3\envs\Bachelor\Lib\site-packages\torch\nn\modules\module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\see\anaconda3\envs\Bachelor\Lib\site-packages\torch\nn\modules\module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\see\anaconda3\envs\Bachelor\Lib\site-packages\diffusers\models\attention.py", line 392, in forward
    ff_output = self.ff(norm_hidden_states)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\see\anaconda3\envs\Bachelor\Lib\site-packages\torch\nn\modules\module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\see\anaconda3\envs\Bachelor\Lib\site-packages\torch\nn\modules\module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\see\anaconda3\envs\Bachelor\Lib\site-packages\diffusers\models\attention.py", line 663, in forward
    hidden_states = module(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\see\anaconda3\envs\Bachelor\Lib\site-packages\torch\nn\modules\module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\see\anaconda3\envs\Bachelor\Lib\site-packages\torch\nn\modules\module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\see\anaconda3\envs\Bachelor\Lib\site-packages\diffusers\models\activations.py", line 105, in forward
    hidden_states = self.proj(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\see\anaconda3\envs\Bachelor\Lib\site-packages\torch\nn\modules\module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\see\anaconda3\envs\Bachelor\Lib\site-packages\torch\nn\modules\module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\see\anaconda3\envs\Bachelor\Lib\site-packages\torch\nn\modules\linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB. GPU